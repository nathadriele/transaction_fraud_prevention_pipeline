{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem para Detecção de Fraudes\n",
    "\n",
    "Este notebook demonstra o desenvolvimento e avaliação de modelos de machine learning para detecção de fraudes transacionais.\n",
    "\n",
    "## Objetivos\n",
    "- Preprocessar dados para modelagem\n",
    "- Treinar modelos supervisionados e não supervisionados\n",
    "- Avaliar performance dos modelos\n",
    "- Comparar diferentes abordagens\n",
    "- Interpretar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações necessárias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import joblib\n",
    "\n",
    "# Adiciona o diretório raiz ao path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Importações do projeto\n",
    "from src.data.data_loader import DataLoader\n",
    "from src.data.preprocessor import DataPreprocessor\n",
    "from src.models.supervised_models import SupervisedModels\n",
    "from src.models.unsupervised_models import UnsupervisedModels\n",
    "from src.utils.helpers import format_percentage\n",
    "\n",
    "# Configurações\n",
    "warnings.filterwarnings('ignore')\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    plt.style.use('default')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"✅ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento e Preprocessamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega dados\n",
    "print(\"Carregando dados...\")\n",
    "loader = DataLoader()\n",
    "users_df, transactions_df = loader.load_synthetic_data()\n",
    "\n",
    "print(f\"Dados carregados:\")\n",
    "print(f\"   - Usuários: {len(users_df):,}\")\n",
    "print(f\"   - Transações: {len(transactions_df):,}\")\n",
    "print(f\"   - Taxa de fraude: {transactions_df['is_fraud'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessamento\n",
    "print(\"Preprocessando dados...\")\n",
    "preprocessor = DataPreprocessor()\n",
    "processed_df = preprocessor.fit_transform(transactions_df)\n",
    "\n",
    "print(f\"Dados processados:\")\n",
    "print(f\"   - Shape original: {transactions_df.shape}\")\n",
    "print(f\"   - Shape processado: {processed_df.shape}\")\n",
    "print(f\"   - Features criadas: {processed_df.shape[1] - transactions_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divisão treino/teste\n",
    "X_train, X_test, y_train, y_test = preprocessor.prepare_train_test_split(processed_df)\n",
    "\n",
    "print(f\"Divisão dos dados:\")\n",
    "print(f\"   - Treino: {X_train.shape[0]:,} amostras\")\n",
    "print(f\"   - Teste: {X_test.shape[0]:,} amostras\")\n",
    "print(f\"   - Features: {X_train.shape[1]}\")\n",
    "print(f\"   - Taxa de fraude (treino): {y_train.mean():.2%}\")\n",
    "print(f\"   - Taxa de fraude (teste): {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Supervisionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa e treina modelos supervisionados\n",
    "print(\"Treinando modelos supervisionados...\")\n",
    "supervised_models = SupervisedModels()\n",
    "\n",
    "# Treina com balanceamento SMOTE\n",
    "training_results = supervised_models.train_models(\n",
    "    X_train, y_train, \n",
    "    balance_method='smote',\n",
    "    use_cross_validation=True\n",
    ")\n",
    "\n",
    "print(\"\\nResultados do treinamento:\")\n",
    "for model_name, results in training_results.items():\n",
    "    if 'error' not in results:\n",
    "        print(f\"   - {model_name}: CV Score = {results['cv_mean']:.4f} (+/- {results['cv_std']:.4f})\")\n",
    "    else:\n",
    "        print(f\"   - {model_name}: ERRO - {results['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia modelos no conjunto de teste\n",
    "print(\"Avaliando modelos no conjunto de teste...\")\n",
    "evaluation_results = supervised_models.evaluate_models(X_test, y_test)\n",
    "\n",
    "# Cria DataFrame com resultados\n",
    "results_data = []\n",
    "for model_name, metrics in evaluation_results.items():\n",
    "    if 'error' not in metrics:\n",
    "        results_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'F1-Score': metrics['f1_score'],\n",
    "            'ROC-AUC': metrics.get('roc_auc', 'N/A')\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(\"\\nMétricas de Avaliação:\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização das métricas\n",
    "if len(results_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Precision\n",
    "    results_df.plot(x='Model', y='Precision', kind='bar', ax=axes[0,0], color='skyblue')\n",
    "    axes[0,0].set_title('Precision por Modelo')\n",
    "    axes[0,0].set_ylabel('Precision')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Recall\n",
    "    results_df.plot(x='Model', y='Recall', kind='bar', ax=axes[0,1], color='lightcoral')\n",
    "    axes[0,1].set_title('Recall por Modelo')\n",
    "    axes[0,1].set_ylabel('Recall')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # F1-Score\n",
    "    results_df.plot(x='Model', y='F1-Score', kind='bar', ax=axes[1,0], color='lightgreen')\n",
    "    axes[1,0].set_title('F1-Score por Modelo')\n",
    "    axes[1,0].set_ylabel('F1-Score')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # ROC-AUC (apenas para modelos que têm essa métrica)\n",
    "    roc_data = results_df[results_df['ROC-AUC'] != 'N/A'].copy()\n",
    "    if len(roc_data) > 0:\n",
    "        roc_data['ROC-AUC'] = pd.to_numeric(roc_data['ROC-AUC'])\n",
    "        roc_data.plot(x='Model', y='ROC-AUC', kind='bar', ax=axes[1,1], color='gold')\n",
    "        axes[1,1].set_title('ROC-AUC por Modelo')\n",
    "        axes[1,1].set_ylabel('ROC-AUC')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Melhor modelo\n",
    "try:\n",
    "    best_model = supervised_models.get_best_model()\n",
    "    print(f\"\\nMelhor modelo: {best_model}\")\n",
    "    print(f\"   - F1-Score: {evaluation_results[best_model]['f1_score']:.4f}\")\n",
    "    print(f\"   - Precision: {evaluation_results[best_model]['precision']:.4f}\")\n",
    "    print(f\"   - Recall: {evaluation_results[best_model]['recall']:.4f}\")\n",
    "except:\n",
    "    print(\"\\n❌ Nenhum modelo foi treinado com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de Curvas ROC e Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas ROC e Precision-Recall\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Curva ROC\n",
    "for model_name in supervised_models.trained_models.keys():\n",
    "    try:\n",
    "        y_pred_proba = supervised_models.predict(X_test, model_name, return_probabilities=True)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        axes[0].plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})')\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao plotar ROC para {model_name}: {e}\")\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('Curva ROC')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Curva Precision-Recall\n",
    "for model_name in supervised_models.trained_models.keys():\n",
    "    try:\n",
    "        y_pred_proba = supervised_models.predict(X_test, model_name, return_probabilities=True)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        ap_score = average_precision_score(y_test, y_pred_proba)\n",
    "        \n",
    "        axes[1].plot(recall, precision, label=f'{model_name} (AP = {ap_score:.3f})')\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao plotar PR para {model_name}: {e}\")\n",
    "\n",
    "baseline = y_test.mean()\n",
    "axes[1].axhline(y=baseline, color='k', linestyle='--', label=f'Baseline ({baseline:.3f})')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Curva Precision-Recall')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Não Supervisionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina modelos não supervisionados\n",
    "print(\"🔍 Treinando modelos não supervisionados...\")\n",
    "try:\n",
    "    unsupervised_models = UnsupervisedModels()\n",
    "    \n",
    "    # Remove target para treino não supervisionado\n",
    "    X_unsupervised = X_train.copy()\n",
    "    \n",
    "    # Treina todos os modelos\n",
    "    unsupervised_results = unsupervised_models.train_all_models(X_unsupervised)\n",
    "    \n",
    "    print(\"\\n📊 Resultados dos modelos não supervisionados:\")\n",
    "    for model_name, results in unsupervised_results.items():\n",
    "        if 'error' not in results:\n",
    "            anomaly_rate = results.get('anomaly_rate', 0)\n",
    "            print(f\"   - {model_name}: Taxa de anomalias = {anomaly_rate:.2%}\")\n",
    "        else:\n",
    "            print(f\"   - {model_name}: ERRO - {results['error']}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro ao treinar modelos não supervisionados: {e}\")\n",
    "    unsupervised_models = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia modelos não supervisionados com labels (se disponível)\n",
    "if unsupervised_models is not None:\n",
    "    try:\n",
    "        print(\"📈 Avaliando modelos não supervisionados com labels...\")\n",
    "        unsupervised_evaluation = unsupervised_models.evaluate_with_labels(X_test, y_test)\n",
    "        \n",
    "        # Cria DataFrame com resultados\n",
    "        unsup_results_data = []\n",
    "        for model_name, metrics in unsupervised_evaluation.items():\n",
    "            if 'error' not in metrics:\n",
    "                unsup_results_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'Precision': metrics['precision'],\n",
    "                    'Recall': metrics['recall'],\n",
    "                    'F1-Score': metrics['f1_score'],\n",
    "                    'Anomaly Rate': metrics['anomaly_rate']\n",
    "                })\n",
    "        \n",
    "        if unsup_results_data:\n",
    "            unsup_results_df = pd.DataFrame(unsup_results_data)\n",
    "            print(\"\\n📊 Métricas dos Modelos Não Supervisionados:\")\n",
    "            print(unsup_results_df.round(4))\n",
    "        else:\n",
    "            print(\"\\n❌ Nenhum modelo não supervisionado foi avaliado com sucesso\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erro na avaliação: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Importância das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise de importância das features (para modelos que suportam)\n",
    "try:\n",
    "    best_model = supervised_models.get_best_model()\n",
    "    feature_names = X_train.columns.tolist()\n",
    "    \n",
    "    feature_importance = supervised_models.get_feature_importance(best_model, feature_names)\n",
    "    \n",
    "    if feature_importance:\n",
    "        # Top 15 features mais importantes\n",
    "        top_features = dict(list(feature_importance.items())[:15])\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        features = list(top_features.keys())\n",
    "        importances = list(top_features.values())\n",
    "        \n",
    "        plt.barh(range(len(features)), importances)\n",
    "        plt.yticks(range(len(features)), features)\n",
    "        plt.xlabel('Importância')\n",
    "        plt.title(f'Top 15 Features Mais Importantes - {best_model}')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n🎯 Top 10 features mais importantes ({best_model}):\")\n",
    "        for i, (feature, importance) in enumerate(list(feature_importance.items())[:10]):\n",
    "            print(f\"   {i+1:2d}. {feature}: {importance:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\n❌ Modelo {best_model} não suporta análise de importância\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro na análise de importância: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 Salvando Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva modelos treinados\n",
    "print(\"💾 Salvando modelos...\")\n",
    "try:\n",
    "    saved_paths = supervised_models.save_models()\n",
    "    \n",
    "    print(\"\\n✅ Modelos salvos:\")\n",
    "    for model_name, path in saved_paths.items():\n",
    "        print(f\"   - {model_name}: {path}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro ao salvar modelos: {e}\")\n",
    "\n",
    "# Salva também o preprocessador\n",
    "try:\n",
    "    import joblib\n",
    "    joblib.dump(preprocessor, 'models/preprocessor.pkl')\n",
    "    print(\"   - preprocessor: models/preprocessor.pkl\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro ao salvar preprocessador: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo final dos resultados\n",
    "print(\"📊 RESUMO FINAL - DETECÇÃO DE FRAUDES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n📥 DADOS:\")\n",
    "print(f\"   - Total de transações: {len(transactions_df):,}\")\n",
    "print(f\"   - Taxa de fraude: {transactions_df['is_fraud'].mean():.2%}\")\n",
    "print(f\"   - Features após preprocessamento: {X_train.shape[1]}\")\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    print(f\"\\n🤖 MODELOS SUPERVISIONADOS:\")\n",
    "    best_supervised = results_df.loc[results_df['F1-Score'].idxmax()]\n",
    "    print(f\"   - Melhor modelo: {best_supervised['Model']}\")\n",
    "    print(f\"   - F1-Score: {best_supervised['F1-Score']:.4f}\")\n",
    "    print(f\"   - Precision: {best_supervised['Precision']:.4f}\")\n",
    "    print(f\"   - Recall: {best_supervised['Recall']:.4f}\")\n",
    "    if best_supervised['ROC-AUC'] != 'N/A':\n",
    "        print(f\"   - ROC-AUC: {best_supervised['ROC-AUC']:.4f}\")\n",
    "\n",
    "if 'unsup_results_df' in locals() and len(unsup_results_df) > 0:\n",
    "    print(f\"\\n🔍 MODELOS NÃO SUPERVISIONADOS:\")\n",
    "    best_unsupervised = unsup_results_df.loc[unsup_results_df['F1-Score'].idxmax()]\n",
    "    print(f\"   - Melhor modelo: {best_unsupervised['Model']}\")\n",
    "    print(f\"   - F1-Score: {best_unsupervised['F1-Score']:.4f}\")\n",
    "    print(f\"   - Precision: {best_unsupervised['Precision']:.4f}\")\n",
    "    print(f\"   - Recall: {best_unsupervised['Recall']:.4f}\")\n",
    "\n",
    "print(f\"\\n🎯 PRÓXIMOS PASSOS:\")\n",
    "print(f\"   1. Implementar sistema de regras de negócio\")\n",
    "print(f\"   2. Criar pipeline de predição em tempo real\")\n",
    "print(f\"   3. Desenvolver sistema de monitoramento\")\n",
    "print(f\"   4. Implementar retreinamento automático\")\n",
    "print(f\"   5. Criar alertas e notificações\")\n",
    "\n",
    "print(\"\\n✅ Modelagem concluída com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
