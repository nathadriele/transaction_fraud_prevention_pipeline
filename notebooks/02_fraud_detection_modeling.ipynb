{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelagem para Detec√ß√£o de Fraudes\n",
    "\n",
    "Este notebook demonstra o desenvolvimento e avalia√ß√£o de modelos de machine learning para detec√ß√£o de fraudes transacionais.\n",
    "\n",
    "## Objetivos\n",
    "- Preprocessar dados para modelagem\n",
    "- Treinar modelos supervisionados e n√£o supervisionados\n",
    "- Avaliar performance dos modelos\n",
    "- Comparar diferentes abordagens\n",
    "- Interpretar resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa√ß√µes necess√°rias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import joblib\n",
    "\n",
    "# Adiciona o diret√≥rio raiz ao path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Importa√ß√µes do projeto\n",
    "from src.data.data_loader import DataLoader\n",
    "from src.data.preprocessor import DataPreprocessor\n",
    "from src.models.supervised_models import SupervisedModels\n",
    "from src.models.unsupervised_models import UnsupervisedModels\n",
    "from src.utils.helpers import format_percentage\n",
    "\n",
    "# Configura√ß√µes\n",
    "warnings.filterwarnings('ignore')\n",
    "try:\n",
    "    plt.style.use('seaborn-v0_8')\n",
    "except:\n",
    "    plt.style.use('default')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento e Preprocessamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega dados\n",
    "print(\"Carregando dados...\")\n",
    "loader = DataLoader()\n",
    "users_df, transactions_df = loader.load_synthetic_data()\n",
    "\n",
    "print(f\"Dados carregados:\")\n",
    "print(f\"   - Usu√°rios: {len(users_df):,}\")\n",
    "print(f\"   - Transa√ß√µes: {len(transactions_df):,}\")\n",
    "print(f\"   - Taxa de fraude: {transactions_df['is_fraud'].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessamento\n",
    "print(\"Preprocessando dados...\")\n",
    "preprocessor = DataPreprocessor()\n",
    "processed_df = preprocessor.fit_transform(transactions_df)\n",
    "\n",
    "print(f\"Dados processados:\")\n",
    "print(f\"   - Shape original: {transactions_df.shape}\")\n",
    "print(f\"   - Shape processado: {processed_df.shape}\")\n",
    "print(f\"   - Features criadas: {processed_df.shape[1] - transactions_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divis√£o treino/teste\n",
    "X_train, X_test, y_train, y_test = preprocessor.prepare_train_test_split(processed_df)\n",
    "\n",
    "print(f\"Divis√£o dos dados:\")\n",
    "print(f\"   - Treino: {X_train.shape[0]:,} amostras\")\n",
    "print(f\"   - Teste: {X_test.shape[0]:,} amostras\")\n",
    "print(f\"   - Features: {X_train.shape[1]}\")\n",
    "print(f\"   - Taxa de fraude (treino): {y_train.mean():.2%}\")\n",
    "print(f\"   - Taxa de fraude (teste): {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos Supervisionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa e treina modelos supervisionados\n",
    "print(\"Treinando modelos supervisionados...\")\n",
    "supervised_models = SupervisedModels()\n",
    "\n",
    "# Treina com balanceamento SMOTE\n",
    "training_results = supervised_models.train_models(\n",
    "    X_train, y_train, \n",
    "    balance_method='smote',\n",
    "    use_cross_validation=True\n",
    ")\n",
    "\n",
    "print(\"\\nResultados do treinamento:\")\n",
    "for model_name, results in training_results.items():\n",
    "    if 'error' not in results:\n",
    "        print(f\"   - {model_name}: CV Score = {results['cv_mean']:.4f} (+/- {results['cv_std']:.4f})\")\n",
    "    else:\n",
    "        print(f\"   - {model_name}: ERRO - {results['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia modelos no conjunto de teste\n",
    "print(\"Avaliando modelos no conjunto de teste...\")\n",
    "evaluation_results = supervised_models.evaluate_models(X_test, y_test)\n",
    "\n",
    "# Cria DataFrame com resultados\n",
    "results_data = []\n",
    "for model_name, metrics in evaluation_results.items():\n",
    "    if 'error' not in metrics:\n",
    "        results_data.append({\n",
    "            'Model': model_name,\n",
    "            'Accuracy': metrics['accuracy'],\n",
    "            'Precision': metrics['precision'],\n",
    "            'Recall': metrics['recall'],\n",
    "            'F1-Score': metrics['f1_score'],\n",
    "            'ROC-AUC': metrics.get('roc_auc', 'N/A')\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "print(\"\\nM√©tricas de Avalia√ß√£o:\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza√ß√£o das m√©tricas\n",
    "if len(results_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Precision\n",
    "    results_df.plot(x='Model', y='Precision', kind='bar', ax=axes[0,0], color='skyblue')\n",
    "    axes[0,0].set_title('Precision por Modelo')\n",
    "    axes[0,0].set_ylabel('Precision')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Recall\n",
    "    results_df.plot(x='Model', y='Recall', kind='bar', ax=axes[0,1], color='lightcoral')\n",
    "    axes[0,1].set_title('Recall por Modelo')\n",
    "    axes[0,1].set_ylabel('Recall')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # F1-Score\n",
    "    results_df.plot(x='Model', y='F1-Score', kind='bar', ax=axes[1,0], color='lightgreen')\n",
    "    axes[1,0].set_title('F1-Score por Modelo')\n",
    "    axes[1,0].set_ylabel('F1-Score')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # ROC-AUC (apenas para modelos que t√™m essa m√©trica)\n",
    "    roc_data = results_df[results_df['ROC-AUC'] != 'N/A'].copy()\n",
    "    if len(roc_data) > 0:\n",
    "        roc_data['ROC-AUC'] = pd.to_numeric(roc_data['ROC-AUC'])\n",
    "        roc_data.plot(x='Model', y='ROC-AUC', kind='bar', ax=axes[1,1], color='gold')\n",
    "        axes[1,1].set_title('ROC-AUC por Modelo')\n",
    "        axes[1,1].set_ylabel('ROC-AUC')\n",
    "        axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Melhor modelo\n",
    "try:\n",
    "    best_model = supervised_models.get_best_model()\n",
    "    print(f\"\\nMelhor modelo: {best_model}\")\n",
    "    print(f\"   - F1-Score: {evaluation_results[best_model]['f1_score']:.4f}\")\n",
    "    print(f\"   - Precision: {evaluation_results[best_model]['precision']:.4f}\")\n",
    "    print(f\"   - Recall: {evaluation_results[best_model]['recall']:.4f}\")\n",
    "except:\n",
    "    print(\"\\n‚ùå Nenhum modelo foi treinado com sucesso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An√°lise de Curvas ROC e Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curvas ROC e Precision-Recall\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Curva ROC\n",
    "for model_name in supervised_models.trained_models.keys():\n",
    "    try:\n",
    "        y_pred_proba = supervised_models.predict(X_test, model_name, return_probabilities=True)\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "        \n",
    "        axes[0].plot(fpr, tpr, label=f'{model_name} (AUC = {auc_score:.3f})')\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao plotar ROC para {model_name}: {e}\")\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "axes[0].set_xlabel('False Positive Rate')\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "axes[0].set_title('Curva ROC')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Curva Precision-Recall\n",
    "for model_name in supervised_models.trained_models.keys():\n",
    "    try:\n",
    "        y_pred_proba = supervised_models.predict(X_test, model_name, return_probabilities=True)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        ap_score = average_precision_score(y_test, y_pred_proba)\n",
    "        \n",
    "        axes[1].plot(recall, precision, label=f'{model_name} (AP = {ap_score:.3f})')\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao plotar PR para {model_name}: {e}\")\n",
    "\n",
    "baseline = y_test.mean()\n",
    "axes[1].axhline(y=baseline, color='k', linestyle='--', label=f'Baseline ({baseline:.3f})')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Curva Precision-Recall')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelos N√£o Supervisionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treina modelos n√£o supervisionados\n",
    "print(\"üîç Treinando modelos n√£o supervisionados...\")\n",
    "try:\n",
    "    unsupervised_models = UnsupervisedModels()\n",
    "    \n",
    "    # Remove target para treino n√£o supervisionado\n",
    "    X_unsupervised = X_train.copy()\n",
    "    \n",
    "    # Treina todos os modelos\n",
    "    unsupervised_results = unsupervised_models.train_all_models(X_unsupervised)\n",
    "    \n",
    "    print(\"\\nüìä Resultados dos modelos n√£o supervisionados:\")\n",
    "    for model_name, results in unsupervised_results.items():\n",
    "        if 'error' not in results:\n",
    "            anomaly_rate = results.get('anomaly_rate', 0)\n",
    "            print(f\"   - {model_name}: Taxa de anomalias = {anomaly_rate:.2%}\")\n",
    "        else:\n",
    "            print(f\"   - {model_name}: ERRO - {results['error']}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao treinar modelos n√£o supervisionados: {e}\")\n",
    "    unsupervised_models = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avalia modelos n√£o supervisionados com labels (se dispon√≠vel)\n",
    "if unsupervised_models is not None:\n",
    "    try:\n",
    "        print(\"üìà Avaliando modelos n√£o supervisionados com labels...\")\n",
    "        unsupervised_evaluation = unsupervised_models.evaluate_with_labels(X_test, y_test)\n",
    "        \n",
    "        # Cria DataFrame com resultados\n",
    "        unsup_results_data = []\n",
    "        for model_name, metrics in unsupervised_evaluation.items():\n",
    "            if 'error' not in metrics:\n",
    "                unsup_results_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'Precision': metrics['precision'],\n",
    "                    'Recall': metrics['recall'],\n",
    "                    'F1-Score': metrics['f1_score'],\n",
    "                    'Anomaly Rate': metrics['anomaly_rate']\n",
    "                })\n",
    "        \n",
    "        if unsup_results_data:\n",
    "            unsup_results_df = pd.DataFrame(unsup_results_data)\n",
    "            print(\"\\nüìä M√©tricas dos Modelos N√£o Supervisionados:\")\n",
    "            print(unsup_results_df.round(4))\n",
    "        else:\n",
    "            print(\"\\n‚ùå Nenhum modelo n√£o supervisionado foi avaliado com sucesso\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro na avalia√ß√£o: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Import√¢ncia das Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de import√¢ncia das features (para modelos que suportam)\n",
    "try:\n",
    "    best_model = supervised_models.get_best_model()\n",
    "    feature_names = X_train.columns.tolist()\n",
    "    \n",
    "    feature_importance = supervised_models.get_feature_importance(best_model, feature_names)\n",
    "    \n",
    "    if feature_importance:\n",
    "        # Top 15 features mais importantes\n",
    "        top_features = dict(list(feature_importance.items())[:15])\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        features = list(top_features.keys())\n",
    "        importances = list(top_features.values())\n",
    "        \n",
    "        plt.barh(range(len(features)), importances)\n",
    "        plt.yticks(range(len(features)), features)\n",
    "        plt.xlabel('Import√¢ncia')\n",
    "        plt.title(f'Top 15 Features Mais Importantes - {best_model}')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nüéØ Top 10 features mais importantes ({best_model}):\")\n",
    "        for i, (feature, importance) in enumerate(list(feature_importance.items())[:10]):\n",
    "            print(f\"   {i+1:2d}. {feature}: {importance:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå Modelo {best_model} n√£o suporta an√°lise de import√¢ncia\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro na an√°lise de import√¢ncia: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Salvando Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva modelos treinados\n",
    "print(\"üíæ Salvando modelos...\")\n",
    "try:\n",
    "    saved_paths = supervised_models.save_models()\n",
    "    \n",
    "    print(\"\\n‚úÖ Modelos salvos:\")\n",
    "    for model_name, path in saved_paths.items():\n",
    "        print(f\"   - {model_name}: {path}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao salvar modelos: {e}\")\n",
    "\n",
    "# Salva tamb√©m o preprocessador\n",
    "try:\n",
    "    import joblib\n",
    "    joblib.dump(preprocessor, 'models/preprocessor.pkl')\n",
    "    print(\"   - preprocessor: models/preprocessor.pkl\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao salvar preprocessador: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Resumo Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo final dos resultados\n",
    "print(\"üìä RESUMO FINAL - DETEC√á√ÉO DE FRAUDES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüì• DADOS:\")\n",
    "print(f\"   - Total de transa√ß√µes: {len(transactions_df):,}\")\n",
    "print(f\"   - Taxa de fraude: {transactions_df['is_fraud'].mean():.2%}\")\n",
    "print(f\"   - Features ap√≥s preprocessamento: {X_train.shape[1]}\")\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    print(f\"\\nü§ñ MODELOS SUPERVISIONADOS:\")\n",
    "    best_supervised = results_df.loc[results_df['F1-Score'].idxmax()]\n",
    "    print(f\"   - Melhor modelo: {best_supervised['Model']}\")\n",
    "    print(f\"   - F1-Score: {best_supervised['F1-Score']:.4f}\")\n",
    "    print(f\"   - Precision: {best_supervised['Precision']:.4f}\")\n",
    "    print(f\"   - Recall: {best_supervised['Recall']:.4f}\")\n",
    "    if best_supervised['ROC-AUC'] != 'N/A':\n",
    "        print(f\"   - ROC-AUC: {best_supervised['ROC-AUC']:.4f}\")\n",
    "\n",
    "if 'unsup_results_df' in locals() and len(unsup_results_df) > 0:\n",
    "    print(f\"\\nüîç MODELOS N√ÉO SUPERVISIONADOS:\")\n",
    "    best_unsupervised = unsup_results_df.loc[unsup_results_df['F1-Score'].idxmax()]\n",
    "    print(f\"   - Melhor modelo: {best_unsupervised['Model']}\")\n",
    "    print(f\"   - F1-Score: {best_unsupervised['F1-Score']:.4f}\")\n",
    "    print(f\"   - Precision: {best_unsupervised['Precision']:.4f}\")\n",
    "    print(f\"   - Recall: {best_unsupervised['Recall']:.4f}\")\n",
    "\n",
    "print(f\"\\nüéØ PR√ìXIMOS PASSOS:\")\n",
    "print(f\"   1. Implementar sistema de regras de neg√≥cio\")\n",
    "print(f\"   2. Criar pipeline de predi√ß√£o em tempo real\")\n",
    "print(f\"   3. Desenvolver sistema de monitoramento\")\n",
    "print(f\"   4. Implementar retreinamento autom√°tico\")\n",
    "print(f\"   5. Criar alertas e notifica√ß√µes\")\n",
    "\n",
    "print(\"\\n‚úÖ Modelagem conclu√≠da com sucesso!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
